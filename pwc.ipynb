{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5727fcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-11T07:50:36.173054Z",
     "start_time": "2021-07-11T07:50:34.278801Z"
    },
    "code_folding": [
     4,
     153
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "\n",
    "def hartigan_wong(A, k, iteration=20): \n",
    "\n",
    "    def NC(l): \n",
    "        nc = A[IC1==l].shape[0]\n",
    "        return nc\n",
    "\n",
    "    def D(i, l): \n",
    "        distance = np.sum((centroids[l] - A[i]) ** 2)\n",
    "        return distance\n",
    "\n",
    "    # step4 OPTRA-stage\n",
    "    def step4(IC1, IC2, live, centroids): \n",
    "        tmp_live = []\n",
    "        # for _ in range(iteration): \n",
    "        for i in range(m): \n",
    "            L1 = IC1[i]\n",
    "\n",
    "            R2_list = []\n",
    "            L2_list = []\n",
    "\n",
    "            if L1 in live: \n",
    "                    \n",
    "                for l in range(k): \n",
    "\n",
    "                    # L1以外のクラスターについて計算するため、L1の場合、for文をスキップする\n",
    "                    if l == L1: \n",
    "                        continue\n",
    "\n",
    "                    R2_list.append((NC(l) * D(i, l) / (NC(l) + 1)))\n",
    "                    L2_list.append(l)\n",
    "            else: \n",
    "                for l in range(k): \n",
    "                    if l in live: \n",
    "                        continue\n",
    "                    R2_list.append((NC(l) * D(i, l) / (NC(l) + 1)))\n",
    "                    L2_list.append(l)\n",
    "\n",
    "            # L2がなかったら\n",
    "            if len(L2_list) == 0: \n",
    "                continue\n",
    "\n",
    "            \n",
    "            L2 = L2_list[R2_list.index(min(R2_list))]\n",
    "\n",
    "\n",
    "            # この式を満たす場合、点Iは移動せず、L2がIC2になるだけ\n",
    "            if min(R2_list) >= (NC(L1) * D(i, L1) / (NC(L1) - 1)): \n",
    "                IC2[i] = L2\n",
    "                # このときは移動が起きていないため、liveに追加しない\n",
    "\n",
    "            # このとき、点IはクラスターL2に移動する。そして、L1がIC2になる。\n",
    "            else: \n",
    "                IC1[i] = L2\n",
    "                IC2[i] = L1\n",
    "\n",
    "                # 点の移動が起きたため、クラスター中心を再計算。\n",
    "                for l in range(k): \n",
    "                    centroids[l] = A[IC1 == l].mean(axis=0)\n",
    "\n",
    "                # 移動に関わった2つのクラスターはliveに属する    \n",
    "                if L1 not in tmp_live: \n",
    "                    tmp_live.append(L1)\n",
    "                if L2 not in tmp_live: \n",
    "                    tmp_live.append(L2)\n",
    "\n",
    "        live = tmp_live   \n",
    "        tmp_live = []\n",
    "\n",
    "        if len(live) == 0: \n",
    "            print('break! because the live set is empty!')\n",
    "            return IC1, live, centroids\n",
    "            # break\n",
    "        else: \n",
    "            step6(IC1, IC2, live, centroids)\n",
    "\n",
    "        return IC1, centroids, live\n",
    "\n",
    "    # step6 QTRAN stage\n",
    "    def step6(IC1, IC2, live, centroids):\n",
    "        for i in range(m): \n",
    "            L1 = IC1[i]\n",
    "            L2 = IC2[i]\n",
    "\n",
    "            tmp_IC1 = IC1\n",
    "\n",
    "            R1 = (NC(L1) * D(i, L1)) / (NC(L1) - 1)\n",
    "            R2 = (NC(L2) * D(i, L2)) / (NC(L2) + 1)\n",
    "\n",
    "            if R1 >= R2: \n",
    "                tmp_IC1[i] = L2\n",
    "                IC2[i] = L1\n",
    "\n",
    "                # クラスター中心を更新する\n",
    "                for l in range(k): \n",
    "                    centroids[l] = A[tmp_IC1 == l].mean(axis=0)\n",
    "\n",
    "        if (IC1 == tmp_IC1).all: \n",
    "            IC1 == tmp_IC1\n",
    "            step4(IC1, IC2, live, centroids)\n",
    "        else: \n",
    "            IC1 = tmp_IC1\n",
    "            step6(IC1, IC2, live, centroids)\n",
    "\n",
    "    # インプットのデータより、M(データ数), N(データの次元)を抽出して変数にする\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "\n",
    "    # 初期クラスター中心を求める。平均値で並び替える\n",
    "    # まず、平均値を求める\n",
    "    ave = np.mean(A, axis = 0)\n",
    "    # 平均値からの距離を置いておく配列\n",
    "    d_from_ave = []\n",
    "    for i in range(m): \n",
    "        d_from_ave.append(np.sum((ave - A[i]) ** 2))\n",
    "    # 平均値からの距離で並び替えたindexを保存。\n",
    "    idx = np.argsort(d_from_ave)\n",
    "    centroids = []\n",
    "    # 該当するindexのデータを初期クラスター中心とする。ん\n",
    "    for l in range(k): \n",
    "        lmk = l * (m / k)\n",
    "        c_factor = A[idx==lmk] # クラスター中心\n",
    "        c_factor = list(itertools.chain.from_iterable(c_factor)) # なぜか、二重リストになるので、1重に変更する\n",
    "        c_factor = np.array(c_factor) # ndarrayに変更\n",
    "        centroids.append(c_factor)\n",
    "\n",
    "    # step1 点iが最も近いクラスター中心をIC1, 次に近いものをIC2とする。そして、点iはクラスターIC1に割り当てられる。        \n",
    "    IC1 = np.full(m, 1000)\n",
    "    IC2 = np.full(m, 1000)\n",
    "    # 変更後のIC1を入れておく配列\n",
    "    tmp_IC1 = np.full(m, 1000)\n",
    "\n",
    "    for i in range(m): \n",
    "        distances = np.sum((centroids - A[i]) ** 2, axis=1)\n",
    "        IC1[i] = np.argsort(distances)[0]\n",
    "        IC2[i] = np.argsort(distances)[1]\n",
    "\n",
    "    # step2 クラスター中心を更新する。\n",
    "    for l in range(k): \n",
    "        centroids[l] = A[IC1 == l].mean(axis=0)\n",
    "\n",
    "    # step3 最初は、全てのクラスターがthe live setに属する\n",
    "    live = []\n",
    "    for l in range(k): \n",
    "        live.append(k)\n",
    "    tmp_live = []\n",
    "\n",
    "\n",
    "    IC1, centroids, live = step4(IC1, IC2, live, centroids)\n",
    "    return IC1, centroids, live\n",
    "def hartigan_missing(data, n_clusters, rs_for_initial_values=0):\n",
    "    np.random.seed(rs_for_initial_values)\n",
    "    \n",
    "    #ランダムにクラスタを割り当てる\n",
    "    clusters = np.random.randint(0, n_clusters, data.shape[0])\n",
    "\n",
    "    #nc(クラスタcに属する値の数)\n",
    "    def nc(cluster) : \n",
    "        #nc = len(data[clusters==cluster])\n",
    "        nc = data[clusters==cluster].shape[0]\n",
    "        return nc\n",
    "\n",
    "    #ncj(クラスタcに属する, j列の欠損していない値の数)\n",
    "    def ncj(cluster, j) : \n",
    "        data_cj = data[clusters==cluster][:, j]\n",
    "        complete_data_cj = ~np.isnan(data_cj)\n",
    "        ncj = np.count_nonzero(complete_data_cj)\n",
    "        return ncj\n",
    "\n",
    "    #x_bar cj \n",
    "    def xbar_cj(cluster, j) : \n",
    "        sum_xij = 0\n",
    "        for x in data[clusters==cluster][:, j] : \n",
    "            if math.isnan(x) : #xが欠損値nanの場合にTrueを返す\n",
    "                #sum_xij += 0\n",
    "                pass\n",
    "            else : \n",
    "                sum_xij += x\n",
    "        return sum_xij / ncj(cluster, j)\n",
    "    \n",
    "    #判別式の右辺のシグマ以降を計算する関数\n",
    "    def right_sigma(q, i, frm = 0, to = data.shape[1]):\n",
    "        result = 0;\n",
    "        for j in range(frm, to):\n",
    "            if data[i, j] == np.nan : \n",
    "                #result += 0\n",
    "                pass\n",
    "            else : \n",
    "                result += (xbar_cj(q, j) - data[i, j]) ** 2\n",
    "        return result\n",
    "\n",
    "    #判別式の左辺のシグマ以降を計算する関数\n",
    "    def left_sigma(p, i, frm = 0, to = data.shape[1]):\n",
    "        result = 0\n",
    "        for j in range(frm, to):\n",
    "            if data[i, j] == np.nan : \n",
    "                #result += 0\n",
    "                pass\n",
    "            else : \n",
    "                result += (ncj(p, j) ** 2 / ((ncj(p, j) - 1) ** 2)) * ((xbar_cj(p, j) - data[i, j]) ** 2)\n",
    "        return result\n",
    "    \n",
    "    #アルゴリズムの実行\n",
    "    #個体が一巡する間に入れ替えが起こらなければ終了なので、入れ替えが起こらないときに、カウントする\n",
    "    cnt = 0\n",
    "    while cnt < data.shape[0] : \n",
    "        #判別式の実行\n",
    "        for i, x in enumerate(data) : \n",
    "            #リストを初期化\n",
    "            right_list = []\n",
    "            q_list = []\n",
    "            #pをxの属するクラスタにする\n",
    "            p = clusters[i]\n",
    "            for q in range(n_clusters) : \n",
    "                #pの属さないクラスタに対し、計算を行う\n",
    "                if p != q : \n",
    "                    left = ((nc(p) - 1) / nc(p)) * left_sigma(p, i)\n",
    "                    right = (nc(q) / (nc(q) + 1)) * right_sigma(q, i)\n",
    "                    if left > right : \n",
    "                        #判別式を満たすものをリストに追加する\n",
    "                        right_list.append(right)\n",
    "                        q_list.append(q)\n",
    "            if len(right_list) != 0 : \n",
    "                #リストが空でなければ、リストの最小値のクラスタを割り当てる\n",
    "                clusters[i] = q_list[right_list.index(min(right_list))]\n",
    "                #クラスタが変更されたため、カウントを０にする\n",
    "                cnt = 0\n",
    "            else : \n",
    "                #リストが空ならば、クラスタは変更されない。したがって、カウントする\n",
    "                cnt += 1\n",
    "    centroids = []\n",
    "    for j in range(n_clusters): \n",
    "        centroids.append(data[clusters == j].mean(axis=0))\n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57f342aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-11T07:50:36.974359Z",
     "start_time": "2021-07-11T07:50:36.382082Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"onlinenewspopularity.csv\")\n",
    "# df = data.drop(columns='url')\n",
    "# 各カラム名の先頭にスペースがあったため、削除\n",
    "tmp = []\n",
    "for i in range(len(df.columns)): \n",
    "    tmp.append(df.columns[i].replace(\" \", \"\"))\n",
    "df.columns = tmp\n",
    "\n",
    "df = df.drop(columns=['url', 'timedelta'])\n",
    "\n",
    "# 目的変数を対数変換\n",
    "target = df['shares']\n",
    "target_log = np.log(target)\n",
    "\n",
    "# 対数変換したtargetに、列名をつける\n",
    "target_log = pd.Series(target_log, name='shares_log')\n",
    "# データフレームに追加\n",
    "df = pd.concat([df, target_log], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bd0d6cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-11T07:50:37.495907Z",
     "start_time": "2021-07-11T07:50:37.168015Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# sharesが1400より大きいかどうかで分ける\n",
    "tmp = []\n",
    "# sharesが1400よりも大きい場合、1をtmpにappendする\n",
    "for i in range(len(df)): \n",
    "    if df['shares'][i] > 1400: \n",
    "        tmp.append(1)\n",
    "    else: \n",
    "        tmp.append(0)\n",
    "# 0, 1が入ったリストを、列名をつけてpandas.Seriesにする\n",
    "shares1400 = pd.Series(tmp, name='shares1400')\n",
    "# concatで、dfに結合\n",
    "df = pd.concat([df, shares1400], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6219303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96c9138b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-11T07:50:37.793006Z",
     "start_time": "2021-07-11T07:50:37.744997Z"
    }
   },
   "outputs": [],
   "source": [
    "# dfから、shares, shares_logを削除したものをdf2とする\n",
    "df2 = df.drop(columns=['shares', 'shares_log'])\n",
    "df2_short = df2[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c85ba5fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-11T07:50:38.019693Z",
     "start_time": "2021-07-11T07:50:38.013121Z"
    }
   },
   "outputs": [],
   "source": [
    "df_short = df[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b94cf710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-11T07:49:11.236524Z",
     "start_time": "2021-07-11T07:49:11.221847Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 行列の標準化\n",
    "StandardScaler().fit(df_short)\n",
    "PCA().fit(df_short)\n",
    "pca_cor = PCA().transform(df_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b8dde93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-11T07:49:55.610119Z",
     "start_time": "2021-07-11T07:49:55.598688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.0\n",
       "2    0.0\n",
       "3    0.0\n",
       "4    0.0\n",
       "Name: weekday_is_wednesday, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cfffa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:31:14.634436Z",
     "start_time": "2021-07-05T07:15:53.592311Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6f763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:15:35.495230Z",
     "start_time": "2021-07-05T07:15:35.487121Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24544c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686be95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T07:00:51.749334Z",
     "start_time": "2021-07-05T07:00:51.544803Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b476203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5386c3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2680191a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46911340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed315e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04b7beb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-11T07:46:23.023544Z",
     "start_time": "2021-07-11T07:46:23.017322Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# xとyの散布図の関数\n",
    "def scatter_plot_df(X, Y, data=df): \n",
    "    plt.scatter(x=data[X], y=data[Y])\n",
    "\n",
    "    plt.title('Scatter Plot of {} vs {}'.format(X, Y))\n",
    "    plt.xlabel(X)\n",
    "    plt.ylabel(Y)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51300179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T06:57:41.647473Z",
     "start_time": "2021-07-05T06:57:41.632237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"1. Title: Online News Popularity\\n\\n2. Source Information\\n    -- Creators: Kelwin Fernandes (kafc \\xe2\\x80\\x98@\\xe2\\x80\\x99 inesctec.pt, kelwinfc \\xe2\\x80\\x99@\\xe2\\x80\\x99 gmail.com),\\n                 Pedro Vinagre (pedro.vinagre.sousa \\xe2\\x80\\x99@\\xe2\\x80\\x99 gmail.com) and\\n                 Pedro Sernadela\\n   -- Donor: Kelwin Fernandes (kafc \\xe2\\x80\\x99@\\xe2\\x80\\x99 inesctec.pt, kelwinfc '@' gmail.com)\\n   -- Date: May, 2015\\n\\n3. Past Usage:\\n    1. K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision\\n       Support System for Predicting the Popularity of Online News. Proceedings\\n       of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence,\\n       September, Coimbra, Portugal.\\n\\n       -- Results: \\n          -- Binary classification as popular vs unpopular using a decision\\n             threshold of 1400 social interactions.\\n          -- Experiments with different models: Random Forest (best model),\\n             Adaboost, SVM, KNN and Na\\xc3\\xafve Bayes.\\n          -- Recorded 67% of accuracy and 0.73 of AUC.\\n    - Predicted attribute: online news popularity (boolean)\\n\\n4. Relevant Information:\\n   -- The articles were published by Mashable (www.mashable.com) and their\\n      content as the rights to reproduce it belongs to them. Hence, this\\n      dataset does not share the original content but some statistics\\n      associated with it. The original content be publicly accessed and\\n      retrieved using the provided urls.\\n   -- Acquisition date: January 8, 2015\\n   -- The estimated relative performance values were estimated by the authors\\n      using a Random Forest classifier and a rolling windows as assessment\\n      method.  See their article for more details on how the relative\\n      performance values were set.\\n\\n5. Number of Instances: 39797 \\n\\n6. Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, \\n                             1 goal field)\\n\\n7. Attribute Information:\\n     0. url:                           URL of the article\\n     1. timedelta:                     Days between the article publication and\\n                                       the dataset acquisition\\n     2. n_tokens_title:                Number of words in the title\\n     3. n_tokens_content:              Number of words in the content\\n     4. n_unique_tokens:               Rate of unique words in the content\\n     5. n_non_stop_words:              Rate of non-stop words in the content\\n     6. n_non_stop_unique_tokens:      Rate of unique non-stop words in the\\n                                       content\\n     7. num_hrefs:                     Number of links\\n     8. num_self_hrefs:                Number of links to other articles\\n                                       published by Mashable\\n     9. num_imgs:                      Number of images\\n    10. num_videos:                    Number of videos\\n    11. average_token_length:          Average length of the words in the\\n                                       content\\n    12. num_keywords:                  Number of keywords in the metadata\\n    13. data_channel_is_lifestyle:     Is data channel 'Lifestyle'?\\n    14. data_channel_is_entertainment: Is data channel 'Entertainment'?\\n    15. data_channel_is_bus:           Is data channel 'Business'?\\n    16. data_channel_is_socmed:        Is data channel 'Social Media'?\\n    17. data_channel_is_tech:          Is data channel 'Tech'?\\n    18. data_channel_is_world:         Is data channel 'World'?\\n    19. kw_min_min:                    Worst keyword (min. shares)\\n    20. kw_max_min:                    Worst keyword (max. shares)\\n    21. kw_avg_min:                    Worst keyword (avg. shares)\\n    22. kw_min_max:                    Best keyword (min. shares)\\n    23. kw_max_max:                    Best keyword (max. shares)\\n    24. kw_avg_max:                    Best keyword (avg. shares)\\n    25. kw_min_avg:                    Avg. keyword (min. shares)\\n    26. kw_max_avg:                    Avg. keyword (max. shares)\\n    27. kw_avg_avg:                    Avg. keyword (avg. shares)\\n    28. self_reference_min_shares:     Min. shares of referenced articles in\\n                                       Mashable\\n    29. self_reference_max_shares:     Max. shares of referenced articles in\\n                                       Mashable\\n    30. self_reference_avg_sharess:    Avg. shares of referenced articles in\\n                                       Mashable\\n    31. weekday_is_monday:             Was the article published on a Monday?\\n    32. weekday_is_tuesday:            Was the article published on a Tuesday?\\n    33. weekday_is_wednesday:          Was the article published on a Wednesday?\\n    34. weekday_is_thursday:           Was the article published on a Thursday?\\n    35. weekday_is_friday:             Was the article published on a Friday?\\n    36. weekday_is_saturday:           Was the article published on a Saturday?\\n    37. weekday_is_sunday:             Was the article published on a Sunday?\\n    38. is_weekend:                    Was the article published on the weekend?\\n    39. LDA_00:                        Closeness to LDA topic 0\\n    40. LDA_01:                        Closeness to LDA topic 1\\n    41. LDA_02:                        Closeness to LDA topic 2\\n    42. LDA_03:                        Closeness to LDA topic 3\\n    43. LDA_04:                        Closeness to LDA topic 4\\n    44. global_subjectivity:           Text subjectivity\\n    45. global_sentiment_polarity:     Text sentiment polarity\\n    46. global_rate_positive_words:    Rate of positive words in the content\\n    47. global_rate_negative_words:    Rate of negative words in the content\\n    48. rate_positive_words:           Rate of positive words among non-neutral\\n                                       tokens\\n    49. rate_negative_words:           Rate of negative words among non-neutral\\n                                       tokens\\n    50. avg_positive_polarity:         Avg. polarity of positive words\\n    51. min_positive_polarity:         Min. polarity of positive words\\n    52. max_positive_polarity:         Max. polarity of positive words\\n    53. avg_negative_polarity:         Avg. polarity of negative  words\\n    54. min_negative_polarity:         Min. polarity of negative  words\\n    55. max_negative_polarity:         Max. polarity of negative  words\\n    56. title_subjectivity:            Title subjectivity\\n    57. title_sentiment_polarity:      Title polarity\\n    58. abs_title_subjectivity:        Absolute subjectivity level\\n    59. abs_title_sentiment_polarity:  Absolute polarity level\\n    60. shares:                        Number of shares (target)\\n\\n8. Missing Attribute Values: None\\n\\n9. Class Distribution: the class value (shares) is continuously valued. We\\n                       transformed the task into a binary task using a decision\\n                       threshold of 1400.\\n\\n   Shares Value Range:   Number of Instances in Range:\\n   <  1400            18490\\n   >= 1400            21154\\n\\n\\nSummary Statistics:\\n                       Feature       Min          Max         Mean           SD\\n                     timedelta    8.0000     731.0000     354.5305     214.1611\\n                n_tokens_title    2.0000      23.0000      10.3987       2.1140\\n              n_tokens_content    0.0000    8474.0000     546.5147     471.1016\\n               n_unique_tokens    0.0000     701.0000       0.5482       3.5207\\n              n_non_stop_words    0.0000    1042.0000       0.9965       5.2312\\n      n_non_stop_unique_tokens    0.0000     650.0000       0.6892       3.2648\\n                     num_hrefs    0.0000     304.0000      10.8837      11.3319\\n                num_self_hrefs    0.0000     116.0000       3.2936       3.8551\\n                      num_imgs    0.0000     128.0000       4.5441       8.3093\\n                    num_videos    0.0000      91.0000       1.2499       4.1078\\n          average_token_length    0.0000       8.0415       4.5482       0.8444\\n                  num_keywords    1.0000      10.0000       7.2238       1.9091\\n     data_channel_is_lifestyle    0.0000       1.0000       0.0529       0.2239\\n data_channel_is_entertainment    0.0000       1.0000       0.1780       0.3825\\n           data_channel_is_bus    0.0000       1.0000       0.1579       0.3646\\n        data_channel_is_socmed    0.0000       1.0000       0.0586       0.2349\\n          data_channel_is_tech    0.0000       1.0000       0.1853       0.3885\\n         data_channel_is_world    0.0000       1.0000       0.2126       0.4091\\n                    kw_min_min   -1.0000     377.0000      26.1068      69.6323\\n                    kw_max_min    0.0000  298400.0000    1153.9517    3857.9422\\n                    kw_avg_min   -1.0000   42827.8571     312.3670     620.7761\\n                    kw_min_max    0.0000  843300.0000   13612.3541   57985.2980\\n                    kw_max_max    0.0000  843300.0000  752324.0667  214499.4242\\n                    kw_avg_max    0.0000  843300.0000  259281.9381  135100.5433\\n                    kw_min_avg   -1.0000    3613.0398    1117.1466    1137.4426\\n                    kw_max_avg    0.0000  298400.0000    5657.2112    6098.7950\\n                    kw_avg_avg    0.0000   43567.6599    3135.8586    1318.1338\\n     self_reference_min_shares    0.0000  843300.0000    3998.7554   19738.4216\\n     self_reference_max_shares    0.0000  843300.0000   10329.2127   41027.0592\\n    self_reference_avg_sharess    0.0000  843300.0000    6401.6976   24211.0269\\n             weekday_is_monday    0.0000       1.0000       0.1680       0.3739\\n            weekday_is_tuesday    0.0000       1.0000       0.1864       0.3894\\n          weekday_is_wednesday    0.0000       1.0000       0.1875       0.3903\\n           weekday_is_thursday    0.0000       1.0000       0.1833       0.3869\\n             weekday_is_friday    0.0000       1.0000       0.1438       0.3509\\n           weekday_is_saturday    0.0000       1.0000       0.0619       0.2409\\n             weekday_is_sunday    0.0000       1.0000       0.0690       0.2535\\n                    is_weekend    0.0000       1.0000       0.1309       0.3373\\n                        LDA_00    0.0000       0.9270       0.1846       0.2630\\n                        LDA_01    0.0000       0.9259       0.1413       0.2197\\n                        LDA_02    0.0000       0.9200       0.2163       0.2821\\n                        LDA_03    0.0000       0.9265       0.2238       0.2952\\n                        LDA_04    0.0000       0.9272       0.2340       0.2892\\n           global_subjectivity    0.0000       1.0000       0.4434       0.1167\\n     global_sentiment_polarity   -0.3937       0.7278       0.1193       0.0969\\n    global_rate_positive_words    0.0000       0.1555       0.0396       0.0174\\n    global_rate_negative_words    0.0000       0.1849       0.0166       0.0108\\n           rate_positive_words    0.0000       1.0000       0.6822       0.1902\\n           rate_negative_words    0.0000       1.0000       0.2879       0.1562\\n         avg_positive_polarity    0.0000       1.0000       0.3538       0.1045\\n         min_positive_polarity    0.0000       1.0000       0.0954       0.0713\\n         max_positive_polarity    0.0000       1.0000       0.7567       0.2478\\n         avg_negative_polarity   -1.0000       0.0000      -0.2595       0.1277\\n         min_negative_polarity   -1.0000       0.0000      -0.5219       0.2903\\n         max_negative_polarity   -1.0000       0.0000      -0.1075       0.0954\\n            title_subjectivity    0.0000       1.0000       0.2824       0.3242\\n      title_sentiment_polarity   -1.0000       1.0000       0.0714       0.2654\\n        abs_title_subjectivity    0.0000       0.5000       0.3418       0.1888\\n  abs_title_sentiment_polarity    0.0000       1.0000       0.1561       0.2263\\n\\n   \\n Citation Request:\\n \\n Please include this citation if you plan to use this database: \\n \\n    K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision\\n    Support System for Predicting the Popularity of Online News. Proceedings\\n    of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence,\\n    September, Coimbra, Portugal.\"\n"
     ]
    }
   ],
   "source": [
    "f = open(\"/Users/shojiro/desktop/news/OnlineNewsPopularity.names\", \"rb\")\n",
    "news_names = f.read()\n",
    "f.close()\n",
    "print(news_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c95af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
